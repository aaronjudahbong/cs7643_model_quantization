{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kX13KHYv5Xke",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX13KHYv5Xke",
        "outputId": "8557a653-8b16-4f85-bc4f-945fd159f28a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S1pgxqMR5KRk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S1pgxqMR5KRk",
        "outputId": "67d17bdc-0546-4fa0-d135-b518062fb80b"
      },
      "outputs": [],
      "source": [
        "# # This is to copy the files from Drive to the SSD for runtime to make training quicker.\n",
        "!pwd\n",
        "!mkdir -p /content/cs7643_model_quantization/data\n",
        "!cp -r \"/content/drive/MyDrive/cs7643_model_quantization/data/gtFine_trainId\" \"/content/cs7643_model_quantization/data\"\n",
        "!cp -r \"/content/drive/MyDrive/cs7643_model_quantization/data/gtFine_trainIdColorized\" \"/content/cs7643_model_quantization/data\"\n",
        "!cp -r \"/content/drive/MyDrive/cs7643_model_quantization/data/leftImg8bit_trainvaltest.zip\" \"/content/cs7643_model_quantization/data\"\n",
        "!unzip -o /content/cs7643_model_quantization/data/leftImg8bit_trainvaltest.zip -d /content/cs7643_model_quantization/data/leftImg8bit_trainvaltest\n",
        "!rsync -av --exclude='data' --exclude='.*' --exclude='testing.ipynb' --exclude='myvenv' /content/drive/MyDrive/cs7643_model_quantization/ /content/cs7643_model_quantization/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6RpTo3AViuf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6RpTo3AViuf",
        "outputId": "810e35e3-c3e4-48f3-8a92-b0a6681de6a1"
      },
      "outputs": [],
      "source": [
        "%cd /content/cs7643_model_quantization/\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oP0mzxYN5LL-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oP0mzxYN5LL-",
        "outputId": "1796d3bc-ef9b-40c9-9910-2713d632f548"
      },
      "outputs": [],
      "source": [
        "!python -m pipeline.fine_tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qEvusu6OlAtp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEvusu6OlAtp",
        "outputId": "d1c7291a-3975-475e-936d-6f896e8776bb"
      },
      "outputs": [],
      "source": [
        "!python -m src.quantization.qat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81R9ASvl891F",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "81R9ASvl891F",
        "outputId": "525038ca-c567-4415-9675-57b22f4b6b97"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import yaml\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "import json\n",
        "import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.ao.quantization as tq\n",
        "import torch.ao.quantization.quantize_fx as quantize_fx\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "from src.models.deeplabv3_mnv3 import get_empty_model, load_model\n",
        "from src.quantization.quantization_utils import set_seed, build_qconfig\n",
        "from pipeline.create_dataset import cityScapesDataset\n",
        "from pipeline.metrics import calculate_miou\n",
        "\n",
        "\n",
        "def plot_loss(train_losses, val_losses):\n",
        "    fig, ax = plt.subplots()\n",
        "    epochs = range(1, len(train_losses)+1)\n",
        "    ax.plot(epochs, train_losses, marker='o', label='train')\n",
        "    ax.plot(epochs, val_losses, marker='o', label='val')\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.set_title('Training and Validation Loss')\n",
        "    ax.grid()\n",
        "    return fig, ax\n",
        "\n",
        "def plot_miou(mious):\n",
        "    fig, ax = plt.subplots()\n",
        "    epochs = range(1, len(mious)+1)\n",
        "    ax.plot(epochs, mious, marker='o')\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.set_ylabel('mIOU')\n",
        "    ax.set_title('Validation mIOU')\n",
        "    ax.grid()\n",
        "    return fig, ax\n",
        "\n",
        "def run_qat(idx, config):\n",
        "    set_seed()\n",
        "    print(\"--- Running QAT Script ---\")\n",
        "    print(\"Loading Configuration ...\")\n",
        "\n",
        "    device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using Device: {device}\")\n",
        "\n",
        "    print(f\"Loading Model from checkpoint: {config['model_checkpoint']} ...\")\n",
        "    # Get empty model and load checkpoint weights.\n",
        "    model = get_empty_model()\n",
        "    model = load_model(model, config[\"model_checkpoint\"], device=device)\n",
        "    model.eval()\n",
        "    print(f\"Baseline Model Size (MB): {os.path.getsize(config['model_checkpoint']) / 1e6:.2f}\")\n",
        "\n",
        "    # Get Quantization Configuration\n",
        "    print(f\"Building QConfig with mode: {config['mode']}...\")\n",
        "    qconfig_mapping = build_qconfig(\"qat\", config)\n",
        "    print(f\"qconfig_mapping global config: {qconfig_mapping.global_qconfig}\")\n",
        "    print(f\"qconfig_mapping weight config: {qconfig_mapping.global_qconfig.weight}\")\n",
        "    print(f\"qconfig_mapping activation config: {qconfig_mapping.global_qconfig.activation}\")\n",
        "\n",
        "    train_img_path = \"data/leftImg8bit_trainvaltest/leftImg8bit/train\"\n",
        "    train_label_path = \"data/gtFine_trainId/gtFine/train\"\n",
        "    val_img_path = \"data/leftImg8bit_trainvaltest/leftImg8bit/val\"\n",
        "    val_label_path = \"data/gtFine_trainId/gtFine/val\"\n",
        "\n",
        "    cal_dataset = cityScapesDataset(train_img_path, train_label_path, config['training']['train_transforms'])\n",
        "    train_dataset = cityScapesDataset(train_img_path, train_label_path, config['training']['train_transforms'])\n",
        "    val_dataset = cityScapesDataset(val_img_path, val_label_path, config['training']['val_transforms'])\n",
        "\n",
        "    cal_dataloader = DataLoader(cal_dataset, batch_size=2, shuffle=True)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=config['training']['batch_size'], shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=config['training']['batch_size'], shuffle=False)\n",
        "\n",
        "    example_image, _ = next(iter(cal_dataloader))\n",
        "    example_image = example_image.to(device)\n",
        "    prepared_model = quantize_fx.prepare_qat_fx(model, qconfig_mapping, (example_image,))\n",
        "    prepared_model = prepared_model.to(device)\n",
        "    prepared_model.eval()\n",
        "\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=255)\n",
        "    optimizer = optim.Adam(prepared_model.parameters(), lr=float(config['training']['learning_rate']),\n",
        "                                                        weight_decay=float(config['training']['weight_decay']))\n",
        "\n",
        "    print(\"Start Calibration...\")\n",
        "    if config['calibration']['enabled']:\n",
        "        with torch.no_grad():\n",
        "            for i, (image, _) in enumerate(cal_dataloader):\n",
        "                image = image.to(device, non_blocking=True)\n",
        "\n",
        "                prepared_model(image)\n",
        "                if (i % 10 == 0 and i > 0):\n",
        "                    print(f\"  Calibrated {i} batches ...\")\n",
        "\n",
        "                if (i >= config['calibration']['steps'] - 1):\n",
        "                    print(f\"  Completed {config['calibration']['steps']} calibration steps.\")\n",
        "                    break\n",
        "\n",
        "\n",
        "    print(\"Starting QAT...\")\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_mious = []\n",
        "\n",
        "    epochs = config['training']['epochs']\n",
        "    for epoch in range(epochs):\n",
        "        prepared_model.train()\n",
        "        training_loss = 0\n",
        "        for image, label in tqdm(train_dataloader, desc=f\"Training Epoch {epoch}\"):\n",
        "            image = image.to(device, non_blocking=True)\n",
        "            label = label.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = prepared_model(image)['out']\n",
        "            loss = loss_function(out, label)\n",
        "            training_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        prepared_model.eval()\n",
        "        validation_loss = 0\n",
        "        val_miou = 0\n",
        "        with torch.no_grad():\n",
        "            for image, label in tqdm(val_dataloader, desc=f\"Validation Epoch {epoch}\"):\n",
        "                image = image.to(device, non_blocking=True)\n",
        "                label = label.to(device, non_blocking=True)\n",
        "\n",
        "                out = prepared_model(image)['out']\n",
        "                pred = out.argmax(dim=1)\n",
        "                loss = loss_function(out, label)\n",
        "                validation_loss += loss.item()\n",
        "                val_miou += calculate_miou(pred, label)\n",
        "\n",
        "        average_training_loss = training_loss / len(train_dataloader)\n",
        "        average_validation_loss = validation_loss / len(val_dataloader)\n",
        "        average_val_miou = val_miou / len(val_dataloader)\n",
        "\n",
        "        train_losses.append(average_training_loss)\n",
        "        val_losses.append(average_validation_loss)\n",
        "        val_mious.append(average_val_miou)\n",
        "\n",
        "        print(f\"Epoch: {epoch}, Training Loss: {average_training_loss}, Validation Loss: {average_validation_loss}, mIOU: {average_val_miou}\")\n",
        "\n",
        "    print(\"Convert QAT model ...\")\n",
        "    # must move model to CPU to convert, else it errors!\n",
        "    prepared_model = prepared_model.cpu()\n",
        "    quantized_model = quantize_fx.convert_fx(prepared_model.eval())\n",
        "\n",
        "    # print(\"Saving QAT Model ...\")\n",
        "    # torch.save(quantized_model.state_dict(), f\"models/qat_quantized_model{idx}.pth\")\n",
        "    # print(f\"QAT Model Size (MB): {os.path.getsize(f\"models/qat_quantized_model{idx}.pth\") / 1e6:.2f}\")\n",
        "\n",
        "    # save all results\n",
        "    result = {\n",
        "      \"idx\": idx,\n",
        "      \"config\": config,\n",
        "      \"train_losses\": [round(loss, 2) for loss in train_losses],\n",
        "      \"val_losses\": [round(loss, 2) for loss in val_losses],\n",
        "      \"val_mious\": [round(miou, 2) for miou in val_mious],\n",
        "      \"final_train_loss\": train_losses[-1],\n",
        "      \"final_val_loss\": val_losses[-1],\n",
        "      \"final_val_miou\": val_mious[-1]\n",
        "    }\n",
        "\n",
        "    json_path = os.path.join(results_dir, \"qat_results.json\")\n",
        "    if os.path.exists(json_path):\n",
        "      with open(json_path, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "    else:\n",
        "      results = []\n",
        "\n",
        "    results.append(result)\n",
        "\n",
        "    with open(json_path, \"w\") as f:\n",
        "      json.dump(results, f, indent=2)\n",
        "\n",
        "    # save all results\n",
        "    fig, ax = plot_loss(train_losses, val_losses)\n",
        "    plot_path = os.path.join(results_dir, f\"qat_loss_{idx}.png\")\n",
        "    fig.savefig(plot_path)\n",
        "\n",
        "    fig, ax = plot_miou(val_mious)\n",
        "    plot_path = os.path.join(results_dir, f\"miou_{idx}.png\")\n",
        "    fig.savefig(plot_path)\n",
        "\n",
        "### start here... ##################################################################################\n",
        "results_dir = \"results\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "qat_config = {\n",
        "    \"model_checkpoint\": \"models/baseline_init_model.pth\",\n",
        "    \"mode\": ['int8','int6', 'int4'],\n",
        "\n",
        "    \"weights\": {\n",
        "        \"dtype\": \"qint8\",            # Keep constant.\n",
        "        \"granularity\": \"per_channel\" # \"per_channel\" or \"per_tensor\"\n",
        "    },\n",
        "\n",
        "    \"activations\": {\n",
        "        \"dtype\": \"quint8\",        # Keep constant.\n",
        "        \"granularity\": \"per_tensor\",  # Keep constant\n",
        "        \"observer\": [\"histogram\", \"minmax\"]\n",
        "    },\n",
        "\n",
        "    \"calibration\": {\n",
        "        \"enabled\": True,\n",
        "        \"steps\": 10\n",
        "    },\n",
        "\n",
        "    \"training\": {\n",
        "        \"epochs\": 5,\n",
        "        \"batch_size\": [8],\n",
        "        \"learning_rate\": [1e-4, 1e-3],\n",
        "        \"weight_decay\": 1e-5,\n",
        "        \"train_transforms\": {\n",
        "            \"crop\": True,\n",
        "            \"resize\": True,\n",
        "            \"flip\": True\n",
        "        },\n",
        "        \"val_transforms\": {\n",
        "            \"crop\": False,\n",
        "            \"resize\": False,\n",
        "            \"flip\": False\n",
        "        }\n",
        "    },\n",
        "    \"skip_aspp\": [True, False]\n",
        "}\n",
        "\n",
        "modes = qat_config['mode']\n",
        "act_observers = qat_config[\"activations\"][\"observer\"]\n",
        "training_batch_sizes = qat_config[\"training\"][\"batch_size\"]\n",
        "training_learning_rates = qat_config[\"training\"][\"learning_rate\"]\n",
        "skip_aspps = qat_config[\"skip_aspp\"]\n",
        "\n",
        "for i, (mode, act_observer, training_batch_size, training_learning_rate, skip_aspp) in enumerate(itertools.product(modes, act_observers, training_batch_sizes, training_learning_rates, skip_aspps)):\n",
        "  current_config = copy.deepcopy(qat_config)\n",
        "  current_config['mode'] = mode\n",
        "  current_config[\"activations\"][\"observer\"] = act_observer\n",
        "  current_config[\"training\"][\"batch_size\"] = training_batch_size\n",
        "  current_config[\"training\"][\"learning_rate\"] = training_learning_rate\n",
        "  current_config[\"skip_aspp\"] = skip_aspp\n",
        "\n",
        "  print(f\"Run {i} with config: {current_config}\")\n",
        "\n",
        "  run_qat(i, current_config)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
